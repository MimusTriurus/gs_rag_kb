import os
import faiss
import nltk
import joblib
import numpy as np
from pathlib import Path
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer

nltk.download("punkt_tab")

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
DOCUMENTS_PATH = "documents/"
CACHE_DIR = "cache/"
EMBED_MODEL_NAME = "bge-large-en"
GGUF_MODEL_PATH = "model/Meta-Llama-3-8B-Instruct.Q8_0.gguf"
CHUNK_SIZE = 300
TOP_K_CATEGORIES = 3
TOP_K_RETRIEVAL = 5
TOP_K_RERANK = 3

os.makedirs(CACHE_DIR, exist_ok=True)


# === –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ –∏ —á–∞–Ω–∫–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ===

def chunk_text(text, size):
    sentences = nltk.sent_tokenize(text)
    chunks, chunk = [], ""
    for sent in sentences:
        if len(chunk) + len(sent) <= size:
            chunk += " " + sent
        else:
            chunks.append(chunk.strip())
            chunk = sent
    if chunk:
        chunks.append(chunk.strip())
    return chunks


def parse_documents():
    """
    –ß–∏—Ç–∞–µ—Ç .txt —Ñ–∞–π–ª—ã, –≥–¥–µ
     - 1-—è —Å—Ç—Ä–æ–∫–∞ = –∑–∞–≥–æ–ª–æ–≤–æ–∫
     - 2-—è —Å—Ç—Ä–æ–∫–∞ = –ø—É—Ç—å/—Ç—ç–≥–∏ (GameDesign/Combat/Weapons)
     - –¥–∞–ª—å—à–µ = —Ç–µ–ª–æ
    """
    docs_by_category = {}
    for file in Path(DOCUMENTS_PATH).glob("*.txt"):
        with open(file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            if len(lines) < 3:
                continue
            path = lines[1].strip()
            tags = path.split("/")  # —Ç—ç–≥–∏ –∏–∑ –ø—É—Ç–∏
            body = "".join(lines[2:])
            chunks = chunk_text(body, CHUNK_SIZE)
            for tag in tags:
                docs_by_category.setdefault(tag, []).extend(
                    [(chunk, file.name) for chunk in chunks]
                )
    return docs_by_category


# === –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ===

def build_or_load_index(tag, embed_model, docs):
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–≥–∞ —Å–æ–∑–¥–∞—ë–º —Å–≤–æ–π FAISS-–∏–Ω–¥–µ–∫—Å –∏ –∫—ç—à–∏—Ä—É–µ–º:
      embeddings.npy, chunks.pkl, sources.pkl, faiss.index
    """
    tag_dir = os.path.join(CACHE_DIR, tag)
    os.makedirs(tag_dir, exist_ok=True)

    emb_path = os.path.join(tag_dir, "embeddings.npy")
    chunks_path = os.path.join(tag_dir, "chunks.pkl")
    sources_path = os.path.join(tag_dir, "sources.pkl")
    index_path = os.path.join(tag_dir, "faiss.index")

    if all(os.path.exists(p) for p in (emb_path, chunks_path, sources_path, index_path)):
        embeddings = np.load(emb_path)
        chunks = joblib.load(chunks_path)
        sources = joblib.load(sources_path)
        index = faiss.read_index(index_path)
    else:
        chunks, sources = zip(*docs)
        # ‚úÇÔ∏è BGE: –∏—Å–ø–æ–ª—å–∑—É–µ–º BGE –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = embed_model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)

        np.save(emb_path, embeddings)
        joblib.dump(chunks, chunks_path)
        joblib.dump(sources, sources_path)
        faiss.write_index(index, index_path)

    return index, list(chunks), list(sources), embeddings


# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ LLM ===

def select_categories(llm, query, tag_list):
    prompt = (
        f"–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n"
        f"–ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∞–µ—à—å –∑–∞–ø—Ä–æ—Å –∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π.\n"
        f"–í—ã–±–µ—Ä–∏ –¥–æ {TOP_K_CATEGORIES} –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫ –∑–∞–ø—Ä–æ—Å—É.\n\n"
        f"–ó–∞–ø—Ä–æ—Å: ¬´{query}¬ª\n"
        f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(tag_list)}\n"
        f"–û—Ç–≤–µ—Ç (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é):"
    )
    resp = llm(prompt, max_tokens=50, stop=["\n"])
    text = resp["choices"][0]["text"]
    selected = [t.strip() for t in text.split(",") if t.strip() in tag_list]
    return selected[:TOP_K_CATEGORIES]


# === Retrieval & Reranking ===

def retrieve_chunks(index, chunks, sources, query_emb):
    D, I = index.search(query_emb, TOP_K_RETRIEVAL)
    return [(chunks[i], sources[i], float(D[0][j])) for j, i in enumerate(I[0])]


def rerank(llm, query, passages):
    scored = []
    for text, source, _ in passages:
        prompt = (
            f"–û—Ü–µ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∫ –∑–∞–ø—Ä–æ—Å—É –æ—Ç 1 –¥–æ 10.\n\n"
            f"–ó–∞–ø—Ä–æ—Å: {query}\n"
            f"–¢–µ–∫—Å—Ç: {text}\n"
            f"–û—Ü–µ–Ω–∫–∞:"
        )
        out = llm(prompt, max_tokens=5, stop=["\n"])
        try:
            score = float(out["choices"][0]["text"].strip().split()[0])
        except:
            score = 0.0
        scored.append((score, text, source))
    scored.sort(reverse=True, key=lambda x: x[0])
    return scored[:TOP_K_RERANK]


# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ ===

def answer_question(llm, context, query):
    prompt = (
        f"–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n"
        f"–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
        f"–í–æ–ø—Ä–æ—Å: {query}\n–û—Ç–≤–µ—Ç:"
    )
    resp = llm(prompt, max_tokens=256, stop=["\n\n"])
    return resp["choices"][0]["text"].strip()


# === Main ===

def main():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π‚Ä¶")
    # ‚úÇÔ∏è BGE: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SentenceTransformer –Ω–∞ BGE
    embed_model = SentenceTransformer(EMBED_MODEL_NAME)
    llm = Llama(
        model_path=GGUF_MODEL_PATH,
        embedding=False,
        n_ctx=2048,
        n_threads=4,
        verbose=False
    )

    print("–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤‚Ä¶")
    docs_by_category = parse_documents()
    tag_list = list(docs_by_category.keys())
    tag_indexes = {}

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for tag in tag_list:
        idx, chunks, sources, _ = build_or_load_index(tag, embed_model, docs_by_category[tag])
        tag_indexes[tag] = (idx, chunks, sources)

    print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞! –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å.")

    while True:
        query = input("\n–í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ")
        if query.lower() == "exit":
            break

        # 1) –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ BGE
        query_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)

        # 2) –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ LLM
        relevant = select_categories(llm, query, tag_list)
        print(f"‚Üí –í—ã–±—Ä–∞–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(relevant)}")

        # 3) –ü–æ–∏—Å–∫ –∏ —Å–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = []
        for tag in relevant:
            idx, chunks, sources = tag_indexes[tag]
            candidates += retrieve_chunks(idx, chunks, sources, query_emb)

        # 4) Reranking —á–µ—Ä–µ–∑ LLM
        top_passages = rerank(llm, query, candidates)

        # 5) –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        context = "\n---\n".join(f"{src}:\n{text}" for _, text, src in top_passages)
        answer = answer_question(llm, context, query)

        print(f"\nüìö –û—Ç–≤–µ—Ç:\n{answer}")


if __name__ == "__main__":
    main()
